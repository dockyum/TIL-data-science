{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 기사 데이터 수집\n",
    "- scrapy에서 selenium 사용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 프로젝트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'naver_article', using template directory '/opt/anaconda3/lib/python3.8/site-packages/scrapy/templates/project', created in:\r\n",
      "    /Users/dokyum/Workspace/DSS - /TIL-data-science/scrapy/naver_article\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd naver_article\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject naver_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mnaver_article/\u001b[00m\r\n",
      "├── \u001b[01;34mnaver_article\u001b[00m\r\n",
      "│   ├── __init__.py\r\n",
      "│   ├── items.py\r\n",
      "│   ├── middlewares.py\r\n",
      "│   ├── pipelines.py\r\n",
      "│   ├── settings.py\r\n",
      "│   └── \u001b[01;34mspiders\u001b[00m\r\n",
      "│       └── __init__.py\r\n",
      "└── scrapy.cfg\r\n",
      "\r\n",
      "2 directories, 7 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree naver_article/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. xpath 찾기\n",
    "- article links : [not()] 사용\n",
    "- article content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy, requests\n",
    "from scrapy.http import TextResponse\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=101\"\n",
    "headers = {\"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"}\n",
    "req = requests.get(url, headers=headers)\n",
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=009&aid=0004728974')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = response.xpath('//*[@id=\"main_content\"]/div[2]/ul/li/dl/dt[not(@class=\"photo\")]/a/@href').extract()\n",
    "\n",
    "len(links), links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium 사용 : xpath 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements = driver.find_elements_by_xpath('//*[@id=\"main_content\"]/div[2]/ul/li/dl/dt[not(@class=\"photo\")]/a')\n",
    "len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=009&aid=0004728974')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = [element.get_attribute(\"href\") for element in elements]\n",
    "len(links), links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## content : title, category, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=009&aid=0004728974'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = links[0]\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(link, headers=headers)\n",
    "response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"노동·환경·디지털…바이든시대, 무역의 모든 룰이 바뀐다\"',\n",
       " '101',\n",
       " '◆ 매경·한미경제학회 포럼 / 한국경제 정책포럼 ◆    \"코로나19 이후 경기가 자연적으로 회복되기만을 기대해선 안 된다. 뼈를 깎는 각오로 근본 체질을 개혁하지 않으면 한국 경제 침체는 계속될 것이다.\" 길고 길었던 코로나19 터널도 올 들어 끝이 보일 것이란 전망이 나온다. 그렇다면 한국 경제도 자연스러운 경기회복을 기대할 수 있을 것인가. 매일경제와 한미경제학회(KAEA)가 5일(현지시간) 공동 주최한 \\'한국 경제 정책포럼\\'에 참가한 석학들은 \"그렇지 않다\"고 잘라 말했다. 정부가 \\'자연스러운 경기회복의 환상\\'에 빠져 있으면 안 된다는 것이다. 기조발표에 나선 김흥종 대외경제정책연구원(KIEP) 원장·장지상 산업연구원(KIET) 원장·최정표 한국개발연구원(KDI) 원장은 △조 바이든 차기 미국 행정부 하 통상질서 변화 △국내 제조업 구조적 위기 △불평등이 심화된 K자 회복 양상 등을 직시하고 정부가 올해부터 구조조정과 체질 대전환을 냉철히 추진해 나가야 한다고 지적했다. 바이든 행정부 시대 무역환경에 대해 다룬 김 원장은 노동·환경·디지털 세 분야의 \\'삼각 파도\\'가 곧 한국 경제를 덮칠 것으로 내다봤다. 김 원장은 \"현재 한미 자유무역협정(FTA)은 신북미무역협정(USMCA), 미·일 FTA 등과 비교했을 때 상당히 낮은 수준의 무역협정이 됐다\"며 \"노동·환경 등에서 USMCA 수준에 준하는 새로운 FTA를 바이든 행정부가 내놓을 것\"이라고 예상했다. 그러면서 \"정부는 그전에 각국에 국제협력을 제안해 그린산업 분야 관련 기술규제 등에 미리 대비해야 한다\"고 지적했다. 또 그는 \"한국의 디지털 통상환경이 낙후돼 유럽연합(EU) 개인정보보호 규정(GDPR) 등에 제대로 대처하지 못하고 있어 업그레이드가 시급하다\"며 \"얼마 전 통과된 \\'데이터3법\\'에 부족한 개인정보 국외이전 관련 규정을 보완하거나 디지털 통상 관련 통계를 구축하는 작업이 필요하다\"고 강조했다. 김 원장은 바이든 행정부가 \"미국이 중시하는 많은 것을 유보시킨 포괄적·점진적 환태평양경제동반자협정(CPTPP)에 복귀하지는 않을 것\"이라면서 \"대신 세계무역기구(WTO)를 활용하는 등 노동·환경·디지털 분야 등을 강조한 다자무역체제의 틀을 짤 것으로 본다\"고 전망했다. 국내 제조업 위기를 다룬 장 원장은 \"코로나19 확산 국면에서 제조업이 선방한 것처럼 보이지만 이는 \\'착시현상\\'\"이라며 \"제조업 구조조정이 결국 필요하다\"고 역설했다.  그는 \"제조업은 2015년 이후 중국의 수입 대체 정책, 제조업 경쟁력 강화 등 요인에 기반해 조선·철강·기계 등을 중심으로 지속적 약화 추세\"라며 \"코로나19 확산을 극복하더라도 포스트 코로나 시대 제조업 경쟁력은 지금까지 추세대로 하락할 수 있다\"고 경고했다. 이어 \"경기회복 수준에서 한발 더 나아가 \\'바운스 포워드\\'로 가기 위한 산업구조 고도화·경쟁력 강화가 필요하다\"며 \"코로나19가 패러다임 변화의 환경을 제공했으니 올해부터 \\'선도형 경제로 대전환\\'을 위한 초석을 놓아야 한다\"고 조언했다. 최 원장은 한국 경제가 회복 분야와 침체 분야가 극명히 갈리는 \\'K자형 회복\\' 양상을 나타내고 있다며 \\'서비스업 부문 부활\\'을 포스트 코로나 시대 경기회복의 열쇠로 제시했다. 최 원장은 \"고용·투자·소비 부문에서 서비스업 분야는 침체가 특히 심각하다\"며 \"서비스업이 되살아나지 않으면 경기를 되돌리기 굉장히 어려운 상황으로 판단하고 있다\"고 말했다.  최 원장은 이어 \"반면 코로나19 국면을 지나며 비대면 산업이 활황을 타는 등 산업구조가 바뀌고 있다\"며 \"코로나19 이후 구조조정 등으로 산업 재편이 불가피한 상황에서 정부가 어떻게 정책적으로 대응하는지가 중요하다\"고 밝혔다. 그는 \"한국 경제는 2020년 -1.1% 성장이 예상되지만 2021년에는 상품수출 개선 등으로 3.1% 성장할 것으로 전망된다\"며 \"백신·치료제 개발과 보급 속도, 보건 정책과 경기 부양책 간 조화 등에 따라 경기회복 속도는 달라질 수 있다\"고 덧붙였다. 이날 토론자로 나선 서양원 매일경제 전무는 \"한국 경제 회복에서 가장 중요한 건 일자리\"라며 \"스타트업·인공지능(AI)·헬스케어 등 일자리 창출이 가능한 분야에 재정을 투입하고 정부·국회가 기업가 정신을 적극 장려해야 한다. 불필요한 규제 남발도 자제할 필요가 있다\"고 지적했다. 김진일 고려대 교수는 \"EU GDPR로 대표되는 디지털 무역 정책에 대응할 필요성이 크다\"며 \"바이든 행정부 출범 후 디지털 무역 관련 정책이 강화될 것으로 예상되는 만큼 더 대비해야 한다\"고 강조했다. 김대일 서울대 교수는 \"코로나19로 대면 서비스업 종사자, 도소매 업자, 저숙련 근로자 타격이 크다\"며 \"코로나19 극복 이후 성장이 이뤄지면 자연스럽게 해결될 문제지만 정부는 성장률 제고 방향이 아니라 사회복지 정책을 늘리는 쪽으로 갈 가능성이 높아 걱정스럽다\"고 밝혔다.  또 김 교수는 \"낙관론 대부분이 \\'백신이 나왔으니 코로나19가 종식되면 경제도 좋아지겠지\\'라는 생각인데 정말 한국 경제가 좋아질지는 신중히 생각해볼 문제\"라며 \"반등하려면 기업이 주도적으로 치고 나가야 하는데, 취약계층 고용을 담당하는 중소·영세업체나 소상공인에게 얼마나 그런 여력이 있는지는 의문\"이라고 말했다.  [안정훈 기자 / 신혜림 기자] [ⓒ 매일경제 & mk.co.kr, 무단전재 및 재배포 금지]')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = response.xpath('//*[@id=\"articleTitle\"]/text()').extract()[0]\n",
    "category = response.url.split('sid1=')[1].split('&')[0]\n",
    "content = response.xpath('//*[@id=\"articleBodyContents\"]/text()').extract()\n",
    "content = \" \".join(content).strip()\n",
    "title, category, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load naver_article/naver_article/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_article/naver_article/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_article/naver_article/items.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class NaverArticleItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    content = scrapy.Field()\n",
    "    category = scrapy.Field()\n",
    "    link = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. spider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_article/naver_article/spiders/spiders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_article/naver_article/spiders/spiders.py\n",
    "import scrapy\n",
    "from selenium import webdriver\n",
    "from naver_article.items import NaverArticleItem\n",
    "\n",
    "\n",
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = \"NaverArticle\"\n",
    "    allow_domain =[\"https://news.naver.com\"]\n",
    "    \n",
    "    def start_requests(self):\n",
    "        url = \"https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=101\"\n",
    "        \n",
    "        #selenium\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"headless\")\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "\n",
    "        driver.get(url)\n",
    "        print(\"this is urls : {}\".format(url))\n",
    "        ## 오류찾기\n",
    "        \n",
    "        headers = {\"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"}\n",
    "        \n",
    "        elements = driver.find_elements_by_xpath('//*[@id=\"main_content\"]/div[2]/ul/li/dl/dt[not(@class=\"photo\")]/a')\n",
    "        links = [element.get_attribute(\"href\") for element in elements]\n",
    "#         print(\"this is links : {}\".format(links))\n",
    "        \n",
    "        for link in links:\n",
    "            yield scrapy.Request(link, headers=headers, callback=self.parse_content)\n",
    "            \n",
    "    def parse_content(self, response):\n",
    "        item = NaverArticleItem()\n",
    "        item['title'] = response.xpath('//*[@id=\"articleTitle\"]/text()').extract()[0]\n",
    "        item['category'] = response.url.split('sid1=')[1].split('&')[0]\n",
    "        content = response.xpath('//*[@id=\"articleBodyContents\"]/text()').extract() \n",
    "        item['content'] =\" \".join(content).strip()\n",
    "        item['link'] = response.url\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. scrapy crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dokyum/Workspace/DSS - /TIL-data-science/scrapy\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd naver_article && scrapy crawl NaverArticle -o article.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd naver_article && scrapy crawl NaverArticle --logfile NaverArticle.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>101</td>\n",
       "      <td>지난해 서울 강남3구 아파트 매매 4건 중 1건은 외지인이 사들인 '상경투자'인 것...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>작년 강남 아파트 4곳 중 1곳은 외지인이 샀다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>101</td>\n",
       "      <td>경기도 의정부시 장암5구역 재개발 사업 수주전에서 SK건설과 현대엔지니어링 컨소시엄...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>의정부 장암5구역 수주전 치열.. SK컨소, 이주비 대출 80% 지원</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                            content  \\\n",
       "18       101  지난해 서울 강남3구 아파트 매매 4건 중 1건은 외지인이 사들인 '상경투자'인 것...   \n",
       "19       101  경기도 의정부시 장암5구역 재개발 사업 수주전에서 SK건설과 현대엔지니어링 컨소시엄...   \n",
       "\n",
       "                                                 link  \\\n",
       "18  https://news.naver.com/main/read.nhn?mode=LSD&...   \n",
       "19  https://news.naver.com/main/read.nhn?mode=LSD&...   \n",
       "\n",
       "                                     title  \n",
       "18              작년 강남 아파트 4곳 중 1곳은 외지인이 샀다  \n",
       "19  의정부 장암5구역 수주전 치열.. SK컨소, 이주비 대출 80% 지원  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./naver_article/article.csv')\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. pipelines\n",
    "- 크롤링한 데이터를 mongodb에 저장하기 : pymongo 사용\n",
    "- 크롤링한 데이터에서 특정한 키워드가 있는 기사가 수집되면 slack 메신저로 기사 내용과 링크 전송하기\n",
    "    - slack 메시지의 incoming webhook은 1초에 1번 사용가능 > time.sleep(1) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pymongo 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-3.11.2-cp38-cp38-macosx_10_9_x86_64.whl (380 kB)\n",
      "\u001b[K     |████████████████████████████████| 380 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pymongo\n",
      "Successfully installed pymongo-3.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-67-1a86da07e8a4>:3: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  collection.insert({\"title\": \"scrapy\"})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectId('5ff57d03f62040bb9a17a721')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = client.naver_article\n",
    "collection = db.article\n",
    "collection.insert({\"title\": \"scrapy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing naver_article/naver_article/mongodb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_article/naver_article/mongodb.py\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://\")\n",
    "db = client.naver_article\n",
    "collection = db.article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load naver_article/naver_article/pipelines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_article/naver_article/pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_article/naver_article/pipelines.py\n",
    "from itemadapter import ItemAdapter\n",
    "from .mongodb import collection\n",
    "\n",
    "class NaverArticlePipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        \n",
    "        data = { \"title\" : item[\"title\"], \"category\" : item[\"category\"],\"content\" : item[\"content\"],\"link\" : item[\"link\"] }\n",
    "        collection.insert(data)\n",
    "        \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline 사용 설정 : settings.py\n",
    "```\n",
    "ITEM_PIPELINES = {\n",
    "   'naver_article.pipelines.NaverArticlePipeline': 300,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd naver_article && scrapy crawl NaverArticle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
